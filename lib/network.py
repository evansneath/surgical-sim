#!/usr/bin/env python

"""Network module

Contains the neural network class for regression training and testing of the
surgicalsim environment.

Author:
    Evan Sneath - evansneath@gmail.com

License:
    Open Software License v3.0

Classes:
    PathPlanningNetwork: Artificial neural networks class for path prediction.
"""


# Import external modules
import numpy as np

# Import pybrain neural network modules
from pybrain.supervised.trainers.evolino import EvolinoTrainer

from pybrain.structure.networks.recurrent import RecurrentNetwork
from pybrain.structure.modules.lstm import LSTMLayer
from pybrain.structure.modules.linearlayer import LinearLayer
from pybrain.structure.connections.full import FullConnection
from pybrain.structure.modules.module import Module
from pybrain.structure.modules.biasunit import BiasUnit


class PathPlanningNetwork(Module):
    """NeuralNetwork class

    Responsible for training and testing of the neural network for use in
    the surgicalsim testing environment.
    """
    def __init__(self, indim, outdim, hiddim=50):
        """Initialize

        Creates an Evolino neural network and its trainer class object for
        training and testing of datasets.

        Arguments:
            training_data: Supervised learning datasets used as the primary
                training method for the neural network.
        """
        super(NeuralNetwork, self).__init__(indim, outdim, name='PathPlanningNetwork')

        # Create the neural network

        # Instantiate all layers of the network
        self._network = RecurrentNetwork()
        self._in_layer = LinearLayer(indim + outdim)
        self._hid_layer = LSTMLayer(hiddim)
        self._out_layer = LinearLayer(outdim)
        self._bias = BiasUnit()

        # Add layers to the network
        self._network.addInputModule(self._in_layer)
        self._network.addModule(self._hid_layer)
        self._network.addModule(self._bias)
        self._network.addOutputModule(self._out_layer)

        # Instantiate layer connections (in <-> hidden <-> out)
        self._in_to_hid_connection = FullConnection(self._in_layer
                                                    self._hid_layer)
        self._bias_to_hid_connection = FullConnection(self._bias,
                                                      self._hid_layer)
        self._hid_to_out_connection = FullConnection(self._hid_layer,
                                                     self._out_layer)

        # Add layer connections to the network
        self._network.addConnection(self._in_to_hid_connection)
        self._network.addConnection(self._bias_to_hid_connection)
        self._network.addConnection(self._hid_to_out_connection)

        # Create a recurrent connection in the hidden layer
        self._recurrent_connection = FullConnection(self._hid_layer,
                                                    self._hid_layer)
        self._network.addRecurrentConnection(self._recurrent_connection)

        # Reset the network for use
        self._network.sortModules()
        self._network.reset()

        self.offset = self._network.offset
        self.backprojectionFactor = 0.01

        # Create trainer
        self.trainer = EvolinoTrainer(
            evolino_network=self.net,
            dataset=training_data,
            wtRatio=1.0/10.0
        )

        return

    def train(self):
        """Train

        Trains the neural network using the inputted training dataset for
        a single epoch.
        """
        self.trainer.train()
        return

    #def test(self, testing_dataset):
    #    """Test

    #    Given a testing dataset containing input data only, return the
    #    trained results generated by the trained neural network.

    #    Arguments:
    #        testing_dataset: A supervised learning dataset containing only
    #            input testing data.

    #    Returns:
    #        output: The neural network output given the testing dataset.
    #    """
    #    output = self.net.activateOnDataset(testing_dataset)
    #    return output

    def reset(self):
        """Reset

        Resets the neural network to a clean state.
        """
        self._network.reset()
        return

    def washout(self, sequence):
        """Washout
        Force the network to process the sequence instead of the
        backprojection values. Used for adjusting the RNN's state. Returns the
        outputs of the RNN that are needed for linear regression.
        
        Arguments:
            sequence: A sequence of target outputs to set weights

        Returns:
            Raw output for each time step.
        """
        assert len(sequence) != 0
        assert self.outdim == len(sequence[0])

        raw_outputs = []

        for val in sequence:
            # Determine next output based on current output
            backprojection = self._getLastOutput()
            backprojection *= self.backprojectionFactor

            self._activateNetwork(backprojection)

            raw_out = self._getRawOutput()
            raw_outputs.append(raw_out)

            self._setLastOutput(val)

        return np.array(raw_outputs)

    def _activateNetwork(self, input):
        """Activate Neural Network

        Activates the neural network and returns the output based on a given
        input.

        Arguments:
            input: The input array to feed to the input nodes.

        Returns:
            Output array generated by the given input.
        """
        assert len(input) == self._network.indim

        output = np.array(self._network.activate(input))
        self.offset = self._network.offset

        return output

    def activate(self, input):
        """Activate Neural Network

        Wraps the _activateNetwork() method.

        Arguments:
            input: The input array to feed to the input nodes.

        Returns:
            Output array generated by the given input.

        """
        return self._activateNetwork(input)

    def extrapolate(self, sequence, length):
        """Extrapolate Data
        
        Extrapolate 'sequence' for 'length' steps and return the
        extrapolated sequence as array.

        Extrapolating is realized by reseting the network, then washing it out
        with the supplied  sequence, and then generating a sequence.

        Arguments:
            sequence: The initial sequence to use as starting weights.
            length: The length to extrapolate from initial sequence.

        Returns:
            Generated sequence of the size 'length'.
        """
        self.reset()
        self.washout(sequence)
        return self.generate(length)

    def generate(self, length):
        """Generate
        
        Generate a sequence of given length. Use reset() and washout() prior.

        Arguments:
            length: The length of sequence to generate.

        Returns:
            Generated sequence of size 'length'.
        """
        generated_sequence = []

        for _ in xrange(length):
            backprojection = self._getLastOutput()
            backprojection *= self.backprojectionFactor

            out = self._activateNetwork(backprojection)
            generated_sequence.append(out)

        return np.array(generated_sequence)

    def _getLastOutput(self):
        """Get Last Output
        
        Return the current output of the linear output layer.

        Returns:
            The contents of the output buffer.
        """
        if self.offset == 0:
            return np.zeros(self.outdim)

        return self._out_layer.outputbuffer[self.offset - 1]

    def _setLastOutput(self, output):
        """Set Last Output
        
        Force the current output of the linear output layer to 'output'.

        Arguments:
            output: The output to set in the output buffer.
        """
        self._out_layer.outputbuffer[self.offset - 1][:] = output
        return

    #
    # Genome related
    #

    def _validateGenomeLayer(self, layer):
        """Validate the type and state of a layer."""
        assert isinstance(layer, LSTMLayer)
        assert not layer.peepholes
        return

    def getGenome(self):
        """Return the RNN's Genome."""
        return self._getGenomeOfLayer(self._hid_layer)

    def setGenome(self, weights):
        """Set the RNN's Genome."""
        weights = deepcopy(weights)
        self._setGenomeOfLayer(self._hid_layer, weights)

    def _getGenomeOfLayer(self, layer):
        """Return the genome of a single layer."""
        self._validateGenomeLayer(layer)

        connections = self._getInputConnectionsOfLayer(layer)

        layer_weights = []
        # iterate cells of layer
        for cell_idx in range(layer.outdim):
            # todo: the evolino paper uses a different order of weights for the genotype of a lstm cell
            cell_weights = []
            # iterate weight types (ingate, forgetgate, cell and outgate)
            for t in range(4):
                # iterate connections
                for c in connections:
                    # iterate sources of connection
                    for i in range(c.indim):
                        idx = i + cell_idx * c.indim + t * layer.outdim * c.indim
                        cell_weights.append(c.params[idx])

            layer_weights.append(cell_weights)

        return layer_weights

    def _setGenomeOfLayer(self, layer, weights):
        """Set the genome of a single layer."""
        self._validateGenomeLayer(layer)

        connections = self._getInputConnectionsOfLayer(layer)

        # iterate cells of layer
        for cell_idx in range(layer.outdim):
            # todo: the evolino paper uses a different order of weights for the genotype of a lstm cell
            cell_weights = weights[cell_idx]

            # iterate weight types (ingate, forgetgate, cell and outgate)
            for t in range(4):
                # iterate connections
                for c in connections:
                    # iterate sources of connection
                    for i in range(c.indim):
                        idx = i + cell_idx * c.indim + t * layer.outdim * c.indim
                        c.params[idx] = cell_weights.pop(0)

        return

    #
    #  Linear Regression related
    #

    def setOutputWeightMatrix(self, W):
        """Set the weight matrix of the linear output layer."""
        
        self._hid_to_out_connection.params[:] = W.flatten()

        return

    def getOutputWeightMatrix(self):
        """Return the weight matrix of the linear output layer."""
        p = self._hid_to_out_connection.params

        return np.reshape(p, (c.outdim, c.indim))

    def _getRawOutput(self):
        """Return the current output of the RNN. This is needed for linear
        regression, which calculates the weight matrix of the linear output
        layer."""
        return np.copy(self._hid_layer.outputbuffer[self.offset - 1])

    #
    # Topology Helper
    #

    def _getInputConnectionsOfLayer(self, layer):
        """Return a list of all input connections for the layer."""
        connections = []
        all_cons = list(self._network.recurrentConns)
        all_cons += np.sum(self._network.connections.values(), [])

        for c in all_cons:
            if c.outmod is layer:
                if not isinstance(c, FullConnection):
                    raise NotImplementedError(
                        "Only FullConnections are supported")

                connections.append(c)

        return connections


if __name__ == '__main__':
    pass
